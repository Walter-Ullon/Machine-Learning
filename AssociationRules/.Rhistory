AR_n = FALSE, powerspectrum = FALSE)
library("earlywarnings", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library("ggplot2", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library("earlywarnings", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library("ggplot2", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
SEED0 = read.csv("Gillespie_SIR(N3x10to5)Seed0.csv")
generic_ews(SEED0, winsize = 25, detrending = c("no", "gaussian",
"loess", "linear", "first-diff"), bandwidth = NULL, span = NULL,
degree = NULL, logtransform = FALSE, interpolate = FALSE,
AR_n = FALSE, powerspectrum = FALSE)
SEED0 = read.csv("Gillespie_SIR(N3x10to5)Seed0.csv")
generic_ews(SEED0, winsize = 25, detrending = c("no", "gaussian",
"loess", "linear", "first-diff"), bandwidth = NULL, span = NULL,
degree = NULL, logtransform = FALSE, interpolate = FALSE,
AR_n = FALSE, powerspectrum = FALSE)
library("earlywarnings", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library("ggplot2", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
install.packages(c("MASS", "Matrix", "mgcv", "nlme", "sp", "spatial"))
library("earlywarnings", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
install.packages("ggplot2")
library("earlywarnings", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
SEED0 = read.csv("Gillespie_SIR(N3x10to5)Seed0.csv")
generic_ews(SEED0, winsize = 25, detrending = c("no", "gaussian",
"loess", "linear", "first-diff"), bandwidth = NULL, span = NULL,
degree = NULL, logtransform = FALSE, interpolate = FALSE,
AR_n = FALSE, powerspectrum = FALSE)
install.packages("earlywarnings")
install.packages("ggplot2")
library("earlywarnings", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library("earlywarnings", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library("ggplot2", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
SEED0 = read.csv("Gillespie_SIR(N3x10to5)Seed0.csv")
generic_ews(SEED0, winsize = 25, detrending = c("no", "gaussian",
"loess", "linear", "first-diff"), bandwidth = NULL, span = NULL,
degree = NULL, logtransform = FALSE, interpolate = FALSE,
AR_n = FALSE, powerspectrum = FALSE)
remove.packages(c("ggplot2", "data.table"))
install.packages('Rcpp', dependencies = TRUE)
install.packages('ggplot2', dependencies = TRUE)
install.packages('data.table', dependencies = TRUE)
install.packages("Rcpp", dependencies = TRUE)
library("earlywarnings", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library("ggplot2", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
SEED0 = read.csv("Gillespie_SIR(N3x10to5)Seed0.csv")
generic_ews(SEED0, winsize = 25, detrending = c("no", "gaussian",
"loess", "linear", "first-diff"), bandwidth = NULL, span = NULL,
degree = NULL, logtransform = FALSE, interpolate = FALSE,
AR_n = FALSE, powerspectrum = FALSE)
detach("package:earlywarnings", unload=TRUE)
library("earlywarnings", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
detach("package:ggplot2", unload=TRUE)
library("ggplot2", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
View(E)
load("~/.RData")
install.packages(c("acepack", "akima", "bit64", "boot", "car", "caret", "chron", "cluster", "codetools", "colorspace", "crayon", "curl", "data.table", "digest", "evaluate", "fastmatch", "fields", "foreign", "formatR", "Formula", "gdata", "ggplot2", "gridExtra", "gtable", "highlight", "highr", "Hmisc", "ifultools", "knitr", "lattice", "latticeExtra", "lme4", "lmtest", "mapproj", "maps", "maptools", "markdown", "MASS", "Matrix", "memoise", "mgcv", "mime", "multcomp", "munsell", "mvtnorm", "nlme", "nnet", "pbkrtest", "pkgKitten", "plm", "plyr", "quantreg", "Rcpp", "RcppEigen", "reshape", "reshape2", "rpart", "sapa", "scales", "scatterplot3d", "som", "sp", "spam", "SparseM", "splus2R", "stringi", "stringr", "survival", "testthat", "tgp", "tseries", "wmtsa", "xts", "yaml", "zoo"))
clear
install.packages("shiny")
library("ggplot2", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library("shiny", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
install.packages("RWeka")
install.packages("RWeka")
library("RWekajars", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library(RWeka)
install.packages("rJava",type = "source")
install.packages("rJava", type = "source")
install.packages("RWeka")
sudo R CMD javareconf
$ sudo R CMD javareconf
sudo R
library("RWekajars", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
install.packages("rJava",type='source')
install.packages("rJava", type = "source")
library(RWeka)
install.packages("RWeka")
library(RWeka)
library("RWeka", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library("RWekajars", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library("RWeka", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
install.packages("RWeka")
install.packages("Rcpp")
library("RWeka", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library("RWekajars", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
install.packages("rJava",type='source')
install.packages("RWeka")
library(RWeka)
library("RWeka", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library("RWekajars", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
detach("package:RWeka", unload=TRUE)
library("RWeka", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
detach("package:RWekajars", unload=TRUE)
library("RWekajars", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library(RWeka)
library("RWeka", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library("RWekajars", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
print(mydata)
patients <- c("John Doe", "Jane Doe", "Steve Graves")
temps <- c(98.1, 98.6, 101.4)
flu <- c(FALSE, FALSE, TRUE)
mydata <- data.frame(patients, temps, flu)
print(mydata)
patients <- c("John Doe", "Jane Doe", "Steve Graves")
temps <- c(98.1, 98.6, 101.4)
fluStatus <- c(FALSE, FALSE, TRUE)
gender <- factor(c("MALE", "FEMALE", "MALE"))
bloodType <- factor(c("O", "AB", "A"), levels = c("A", "B", "AB", "O"))
mydata <- data.frame(patients, gender,temps, bloodType,fluStatus)
print(mydata)
patients <- c("John Doe", "Jane Doe", "Steve Graves")
temps <- c(98.1, 98.6, 101.4)
fluStatus <- c(FALSE, FALSE, TRUE)
gender <- factor(c("MALE", "FEMALE", "MALE"))
bloodType <- factor(c("O", "AB", "A"),
levels = c("A", "B", "AB", "O"))
symptoms <- factor(c("MILD", "MODERATE", "SEVERE"),
levels = c("MILD", "MODERATE", "SEVERE"),
ordered = TRUE)
mydata <- data.frame(patients, gender,temps, bloodType,symptoms, fluStatus)
print(mydata)
patients <- c("John Doe", "Jane Doe", "Steve Graves")
symptoms
subject1
subject1 <- list(fullName = patients[1],
temps = temps[1],
fluStatus = fluStatus[1],
gender = gender[1],
bloodType = bloodType[1],
symptoms = symptoms[1])
subject1
mydata <- data.frame(patients, gender,temps, bloodType,symptoms, fluStatus, stringsAsFactors = FALSE)
print(mydata)
ls()
usedCars <-read.csv(url("https://raw.githubusercontent.com/dataspelunking/MLwR/master/Machine%20Learning%20with%20R%20(2nd%20Ed.)/Chapter%2002/usedcars.csv"))
print(usedCars)
usedCars <-read.csv(url("https://raw.githubusercontent.com/dataspelunking/MLwR/master/Machine%20Learning%20with%20R%20(2nd%20Ed.)/Chapter%2002/usedcars.csv"), stringsAsFactors = FALSE)
print(usedCars)
print(usedCars)
str(usedCars)
summary(usedCars$year)
summary(usedCars[c("price", "mileage")])
boxplot(usedCars[c("price", "mileage")])
boxplot(usedCars$price, main="boxplot of used cars price", ylab="Prices ($)")
?boxplot
hist(usedCars$price, main="histogram of used car prices", xlab="Price ($)")
table(usedCars$color)
prop.table(usedCars$color)
prop.table(usedCars$color)
model_table <- table(usedCars$color)
prop.table(model_table)
plot(x = usedCars$mileage, y = usedCars$price, main = "Mileage vs. Price", xlab = "mileage", ylab = "price")
installed.packages("gmodels")
library(gmodels)
install.packages("gmodels")
library(gmodels)
usedCars$conservative <- usedCars$color %in% c("Black", "Gray", "Silver", "White")
table(usedCars$conservative)
CrossTable(x = usedCars$model, y = usedCars$conservative)
CrossTable(x = usedCars$model, y = usedCars$conservative, chisq = TRUE)
wbcd <- read.csv(url("https://raw.githubusercontent.com/dataspelunking/MLwR/master/Machine%20Learning%20with%20R%20(2nd%20Ed.)/Chapter%2003/wisc_bc_data.csv"))
str(wbcd)
print(wbcd[1,])
wbcd <- wbcd(-1)
wbcd <- read.csv(url("https://raw.githubusercontent.com/dataspelunking/MLwR/master/Machine%20Learning%20with%20R%20(2nd%20Ed.)/Chapter%2003/wisc_bc_data.csv"))
str(wbcd)
wbcd <- wbcd(-1)
wbcd <- read.csv(url("https://raw.githubusercontent.com/dataspelunking/MLwR/master/Machine%20Learning%20with%20R%20(2nd%20Ed.)/Chapter%2003/wisc_bc_data.csv"))
str(wbcd)
wbcd <- wbcd[-1]
table(wbcd$diagnosis)
wbcd$diagnosis <- factor(wbcd$diagnosis, levels = c("B", "M"),
labels = c("Benign", "Malignant"))
round(prop.table(table(wbcd$diagnosis))*100, digits = 1)
table(wbcd$diagnosis)
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
Normalize(c(1,2,3,4,5))
normalize <- function(x){
return ((x - min(x)) / (max(x) - min(x)))
}
normalize(c(1,2,3,4,5))
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
summary(wbcd_n$area_mean)
wbcd_train <- wbcd_n[1:469,]
wbcd_test <- wbcd_n[470:569,]
wbcn[1:10, 1]
wbcd[1:10, 1]
library("class", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
wbcd_train_labels <- wbcd_n[1:469, 1]
wbcd_test_labels <- wbcd_n[470:569, 1]
wbcd_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k = 21)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k = 21)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq = FALSE)
wbcd <- read.csv(url("https://raw.githubusercontent.com/dataspelunking/MLwR/master/Machine%20Learning%20with%20R%20(2nd%20Ed.)/Chapter%2003/wisc_bc_data.csv"))
str(wbcd)
wbcd <- wbcd[-1]
table(wbcd$diagnosis)
wbcd$diagnosis <- factor(wbcd$diagnosis, levels = c("B", "M"),
labels = c("Benign", "Malignant"))
round(prop.table(table(wbcd$diagnosis))*100, digits = 1)
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
normalize <- function(x){
return ((x - min(x)) / (max(x) - min(x)))
}
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
summary(wbcd_n$area_mean)
wbcd_train <- wbcd[1:469, ]
wbcd_test <- wbcd_n[470:569, ]
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k = 21)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq = FALSE)
wbcd <- read.csv(url("https://raw.githubusercontent.com/dataspelunking/MLwR/master/Machine%20Learning%20with%20R%20(2nd%20Ed.)/Chapter%2003/wisc_bc_data.csv"))
str(wbcd)
wbcd <- wbcd[-1]
table(wbcd$diagnosis)
wbcd$diagnosis <- factor(wbcd$diagnosis, levels = c("B", "M"),
labels = c("Benign", "Malignant"))
round(prop.table(table(wbcd$diagnosis))*100, digits = 1)
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
normalize <- function(x){
return ((x - min(x)) / (max(x) - min(x)))
}
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
summary(wbcd_n$area_mean)
wbcd_train <- wbcd_n[1:469, ]
wbcd_test <- wbcd_n[470:569, ]
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k = 21)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq = FALSE)
wbcd_z <- as.data.frame(scale(wbcd[-1]))
summary(wbcd_z$area_mean)
wbcd_train_z <- wbcd_z[1:469, ]
wbcd_test_z <- wbcd_z[470:569, ]
#import dataset
wbcd <- read.csv(url("https://raw.githubusercontent.com/dataspelunking/MLwR/master/Machine%20Learning%20with%20R%20(2nd%20Ed.)/Chapter%2003/wisc_bc_data.csv"))
#dataset info
str(wbcd)
#remove the unique "ID" feature for the observations (not a meaningful feature)
wbcd <- wbcd[-1]
table(wbcd$diagnosis)
wbcd$diagnosis <- factor(wbcd$diagnosis, levels = c("B", "M"),
labels = c("Benign", "Malignant"))
round(prop.table(table(wbcd$diagnosis))*100, digits = 1)
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
#min-max normalization function
normalize <- function(x){
return ((x - min(x)) / (max(x) - min(x)))
}
#Normalizations using min-max and Z-scores (omit 1st column i.e. "ID")
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
wbcd_z <- as.data.frame(scale(wbcd[-1]))
#confirm normalization is working as intended
summary(wbcd_z$area_mean)
#segmentation of dataset into Training & Test data (for both n and z norms)
wbcd_train <- wbcd_n[1:469, ]
wbcd_test <- wbcd_n[470:569, ]
wbcd_train_z <- wbcd_z[1:469, ]
wbcd_test_z <- wbcd_z[470:569, ]
#extraction of true labels for training and test results
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
#run the KNN algo with k = 21 (sqrt(469)) with normalized data
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k = 21)
#run the KNN algo with k = 21 (sqrt(469)) with z-normalized data
wbcd_test_pred_z <- knn(train = wbcd_train_z, test = wbcd_test_z, cl = wbcd_train_labels, k = 21)
#compare results for both n and z normalized data
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq = FALSE)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred_z, prop.chisq = FALSE)
#import dataset
wbcd <- read.csv(url("https://raw.githubusercontent.com/dataspelunking/MLwR/master/Machine%20Learning%20with%20R%20(2nd%20Ed.)/Chapter%2003/wisc_bc_data.csv"))
#dataset info
str(wbcd)
#remove the unique "ID" feature for the observations (not a meaningful feature)
wbcd <- wbcd[-1]
table(wbcd$diagnosis)
wbcd$diagnosis <- factor(wbcd$diagnosis, levels = c("B", "M"),
labels = c("Benign", "Malignant"))
round(prop.table(table(wbcd$diagnosis))*100, digits = 1)
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
#min-max normalization function
normalize <- function(x){
return ((x - min(x)) / (max(x) - min(x)))
}
#Normalizations using min-max and Z-scores (omit 1st column i.e. "ID")
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
wbcd_z <- as.data.frame(scale(wbcd[-1]))
#confirm normalization is working as intended
summary(wbcd_z$area_mean)
#segmentation of dataset into Training & Test data (for both n and z norms)
wbcd_train <- wbcd_n[1:469, ]
wbcd_test <- wbcd_n[470:569, ]
wbcd_train_z <- wbcd_z[1:469, ]
wbcd_test_z <- wbcd_z[470:569, ]
#extraction of true labels for training and test results
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
#run the KNN algo with k = 21 (sqrt(469)) with normalized data
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k = 21)
#run the KNN algo with k = 21 (sqrt(469)) with z-normalized data
wbcd_test_pred_z <- knn(train = wbcd_train_z, test = wbcd_test_z, cl = wbcd_train_labels, k = 21)
#compare results for both n and z normalized data
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq = FALSE)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred_z, prop.chisq = FALSE)
sqrt(48^2 + 20^2)
for(i in 1:20){}
for(i in 1:20){
print(i)
}
sms_raw <- read.csv(url("https://raw.githubusercontent.com/dataspelunking/MLwR/master/Machine%20Learning%20with%20R%20(2nd%20Ed.)/Chapter%2004/sms_spam.csv"))
str(sms_raw)
sms_raw <- read.csv(url("https://raw.githubusercontent.com/dataspelunking/
MLwR/master/Machine%20Learning%20with%20R%20(2nd%20Ed.)/
Chapter%2004/sms_spam.csv"), stringAsFactors = FALSE)
sms_raw <- read.csv(url("https://raw.githubusercontent.com/dataspelunking/MLwR/master/Machine%20Learning%20with%20R%20(2nd%20Ed.)/Chapter%2004/sms_spam.csv"), stringsAsFactors = FALSE)
str(sms_raw)
sms_raw$type <- factor(sms_raw$type)
str(sms_raw)
str(sms_raw)
table(sms_raw$type)
install.packages("tm")
library(tm)
library("tm", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
install.packages("NLP")
install.packages("NLP")
install.packages("tm")
library(tm)
install.packages("slam")
install.packages('devtools')
library(devtools)
slam_url <- "https://cran.r-project.org/src/contrib/Archive/slam/slam_0.1-37.tar.gz"
install_url(slam_url)
session_info()
updateR()
if(!require(installr)) {
install.packages("installr");
require(installr)
}
updateR()
sessionInfo()
install.packages("tm")
library(tm)
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
print(sms_corpus)
as.character(sms_corpus[[1]])
lapply(sms_corpus[1:3], as.character)
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
lapply(sms_corpus_clean[1:3], as.character)
getTransformations()
?stopwords
sms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords())
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation)
install.packages("SnowballC")
library(SnowballC)
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
lapply(sms_corpus_clean[1:3], as.character)
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)
lapply(sms_corpus_clean[1:3], as.character)
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
print(sms_dtm)
print(sms_dtm[1:3])
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4169:5559, ]
# create training and tests labels (from raw corpus)
sms_train_labels <- sms_raw[1:4169, ]$type
sms_test_labels <- sms_raw[4169:5559, ]$type
# compare proportion of ham vs spam in both sets
prop.table(table(sms_train_labels))
prop.table(table(sms_test_labels))
install.packages("wordcloud")
library(wordcloud)
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE, random.color = TRUE)
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE, random.color = FALSE)
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 40, scale = c(3, 0.5))
spam <- subset(sms_raw, type == "spam")
ham <- subset(sms_raw, type == "ham")
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 40, scale = c(3, 0.5))
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4169:5559, ]
sms_train_labels <- sms_raw[1:4169, ]$type
sms_test_labels <- sms_raw[4169:5559, ]$type
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
str(sms_freq_words)
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)
# import data
sms_raw <- read.csv(url("https://raw.githubusercontent.com/dataspelunking/MLwR/master/Machine%20Learning%20with%20R%20(2nd%20Ed.)/Chapter%2004/sms_spam.csv"), stringsAsFactors = FALSE)
# turn categorical 'type' variable into factor
sms_raw$type <- factor(sms_raw$type)
# get info on imported data
str(sms_raw)
table(sms_raw$type)
# create corpus of SMS messages
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
print(sms_corpus)
# check 1st element of corpus
as.character(sms_corpus[[1]])
# check a list of corpus elements
lapply(sms_corpus[1:3], as.character)
#------------begin cleaning corpus---------------------
# apply lower case transformation to corpus, and check...
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
lapply(sms_corpus_clean[1:3], as.character)
# remove numbers from corpus
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)
# remove stop words from corpus (to, and, but, ...)
sms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords())
# remove punctuation from corpus
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation)
# stem corpus ( learned, learning, learns -> learn)
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
# strip whitespace
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)
#--------------corpus cleaning finalized---------------------
# create a "Document Term Matrix" (DTM). Creates matrix word count per document (SMS).
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
# create training and tests sets
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4169:5559, ]
# create training and tests labels (from raw corpus)
sms_train_labels <- sms_raw[1:4169, ]$type
sms_test_labels <- sms_raw[4169:5559, ]$type
# compare proportion of ham vs spam in both sets
prop.table(table(sms_train_labels))
prop.table(table(sms_test_labels))
#create wordcloud to visulaize frequency of words
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE, random.color = FALSE)
# create and compare wordclouds for ham vs spam (wordcloud applies text prep. processes automatically)
spam <- subset(sms_raw, type == "spam")
ham <- subset(sms_raw, type == "ham")
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 40, scale = c(3, 0.5))
# create a vector with the most frequent words (appearing at least five times)
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
str(sms_freq_words)
sms_dtm_freq_train <- sms_dtm_train[ , sms_freq_words]
sms_dtm_freq_test <- sms_dtm_test[ , sms_freq_words]
1/(1+.05)^-.25
1/(1+.05)^.25
exp(-.05*.25)
250*1.5
375000-361750
1.5-1.4410
.059*250000
setwd("~/Desktop/Machine-Learning/AssociationRules")
# Application of "Association Rule" algorithms to "Market Basket" analysis.
# Data from Packt publishing website.
# Install and load packages.
install.packages("arules")
library(arules)
library(arulesViz)
library(datasets)
# Load data using a "Sparse Matrix". Examine.
groceries <- read.transactions(url("https://raw.githubusercontent.com/dataspelunking/MLwR/master/Machine%20Learning%20with%20R%20(2nd%20Ed.)/Chapter%2008/groceries.csv"), sep = ",")
summary(groceries)
inspect(groceries[1:5])
# Inspect the "support" level of the 1st five items.
itemFrequency(groceries[ , 1:5])
# Create histogram of the items with a minimin support level of 0.1.
pdf("itemSupportLvl.pdf")
itemFrequencyPlot(groceries, support = 0.1)
dev.off()
# Create histogram of the top 20 items.
pdf("itemTop20.pdf")
itemFrequencyPlot(groceries, topN = 20)
dev.off()
# Visualize the Sparse Matrix.
pdf("itemSparseMatrix.pdf")
image(groceries[1:100])
dev.off()
# Create association rules. Summarize.
groceryRules <- apriori(groceries, parameter = list(support = 0.006, confidence = 0.25, minlen = 2))
groceryRules
summary(groceryRules)
# Inspect rules.
inspect(groceryRules[1:3])
# Sort association rules to bring up the more illuminating examples. Sort by "lift".
inspect(sort(groceryRules, by = "lift")[1:5])
# Sort rules by querying specific items into subsets (i.e. berries).
berryRules <- subset(groceryRules, items %in% "berries")
inspect(berryRules)
# Save rules to .CSV file.
write(groceryRules, file = "groceryRules.csv", sep = ",", quote = TRUE, row.names = FALSE)
# Convert rules to dataframe.
groceryRules_df <- as(groceryRules, "data.frame")
plot(berryRules, method="graph")
groceries
head(groceries)
inspect(groceries[1:5])
printSpMatrix(groceries)
show(groceries)
print(groceries)
printSpMatrix(groceries)
formatSpMatrix(groceries)
groceries[1:3]
print(groceries[1:3])
show(groceries[1:3])
sparseVector <- as.data.frame(groceries)
as.matrix(groceries)
groceries$values
perf.@groceries.values
perf@groceries.values
groceries@data
groceries@itemsetInfo
groceries@itemInfo
groceries@x.values
groceries@item.values
groceries@Items.values
groceries@data
groceries@itemsetInfo
groceries@itemInfo
