teens$cluster <- teens_cluster$cluster
aggregate(data = teens, female ~ cluster, mean)
aggregate(data = teens, friends ~ cluster, mean)
library(gmodels)
# Import data from UCI Machine Learning Repository (http://archive.ics.uci.edu/ml)
# Inspect data
credit <- read.csv(url("https://raw.githubusercontent.com/dataspelunking/MLwR/master/Machine%20Learning%20with%20R%20(2nd%20Ed.)/Chapter%2005/credit.csv"))
str(credit)
# Inspect checking and savings balanace as possible indicators of default
table(credit$checking_balance)
table(credit$savings_balance)
# Obtain summary statistics on loan duration and loan amount
summary(credit$months_loan_duration)
summary(credit$amount)
# Obtain total number of defaults
table(credit$default)
# Obtain random sample to split between training and test data. Set seed.
# Returns a vector with 900 random values
set.seed(123)
train_sample <- sample(1000, 900)
str(train_sample)
# Split data into training and test sets.
# Check to see if both sets have ~ proportion of default loans (~30%, as in original set).
credit_train <- credit[train_sample, ]
credit_test <- credit[-train_sample, ]
prop.table(table(credit_train$default))
prop.table(table(credit_test$default))
# Train model.
# Exclude "default" class from training data (17th column) but supply it as target factor vector for classification (labels).
credit_model <- C5.0(credit_train[-17], credit_train$default)
# credit_model # inspect...
# Inspect Tree's decisions and acquire stats on model performance on the training data.
summary(credit_model)
# Apply model to test data and predict...
# Compare prediction against true results.
# Obtain prediction "confidence" (in terms of probabilities).
credit_pred <- predict(credit_model, credit_test)
predicted_prob <- predict(credit_model, credit_test, type = "prob")
CrossTable(credit_test$default, credit_pred, prop.chisq = FALSE, prop.r = FALSE, dnn = c("actual default", "predicted default" ))
library(C50)
library(gmodels)
# Import data from UCI Machine Learning Repository (http://archive.ics.uci.edu/ml)
# Inspect data
credit <- read.csv(url("https://raw.githubusercontent.com/dataspelunking/MLwR/master/Machine%20Learning%20with%20R%20(2nd%20Ed.)/Chapter%2005/credit.csv"))
str(credit)
# Inspect checking and savings balanace as possible indicators of default
table(credit$checking_balance)
table(credit$savings_balance)
# Obtain summary statistics on loan duration and loan amount
summary(credit$months_loan_duration)
summary(credit$amount)
# Obtain total number of defaults
table(credit$default)
# Obtain random sample to split between training and test data. Set seed.
# Returns a vector with 900 random values
set.seed(123)
train_sample <- sample(1000, 900)
str(train_sample)
# Split data into training and test sets.
# Check to see if both sets have ~ proportion of default loans (~30%, as in original set).
credit_train <- credit[train_sample, ]
credit_test <- credit[-train_sample, ]
prop.table(table(credit_train$default))
prop.table(table(credit_test$default))
# Train model.
# Exclude "default" class from training data (17th column) but supply it as target factor vector for classification (labels).
credit_model <- C5.0(credit_train[-17], credit_train$default)
# credit_model # inspect...
# Inspect Tree's decisions and acquire stats on model performance on the training data.
summary(credit_model)
# Apply model to test data and predict...
# Compare prediction against true results.
# Obtain prediction "confidence" (in terms of probabilities).
credit_pred <- predict(credit_model, credit_test)
predicted_prob <- predict(credit_model, credit_test, type = "prob")
CrossTable(credit_test$default, credit_pred, prop.chisq = FALSE, prop.r = FALSE, dnn = c("actual default", "predicted default" ))
#-------------------------------------------------------------------------------------------
# NOTE: up to this point, the model performed rather poorly at identifying true defaults.
#       The proceeding code is intended to "fine-tune" our model to improve its performance.
#       See "Boosting" for more details.
#-------------------------------------------------------------------------------------------
# We employ the same classifying algorith (C5.0) but we "boost" it by adding "trials".
# The trials parameter indicates the additional number of separate decision trees to use. We set trials = 10.
credit_boost10 <- C5.0(credit_train[-17], credit_train$default, trials = 10)
credit_boost10
# Obtain new model's summary and peformance on the test data. Save to .txt file.
sink("credit_boost10.txt")
summary(credit_boost10)
sink()
# Test new model on test data
# Compare predicition against true results.
credit_boost10_pred <- predict(credit_boost10, credit_test)
CrossTable(credit_test$default, credit_boost10_pred, prop.chisq = FALSE, prop.r = FALSE, dnn = c("actual default", "predicted default" ))
predicted_prob <- predict(credit_model, credit_test, type = "prob")
View(predicted_prob)
View(predicted_prob)
# We employ the same classifying algorith (C5.0) but we "boost" it by adding "trials".
# The trials parameter indicates the additional number of separate decision trees to use. We set trials = 10.
credit_boost10 <- C5.0(credit_train[-17], credit_train$default, trials = 10)
credit_boost10
# Obtain new model's summary and peformance on the test data. Save to .txt file.
sink("credit_boost10.txt")
summary(credit_boost10)
sink()
# Test new model on test data
# Compare predicition against true results.
credit_boost10_pred <- predict(credit_boost10, credit_test)
predicted_prob_boost10 <- predict(credit_model, credit_test, type = "prob")
CrossTable(credit_test$default, credit_boost10_pred, prop.chisq = FALSE, prop.r = FALSE, dnn = c("actual default", "predicted default" ))
#-------------------------------------------------------------------------------------------
# NOTE: the model is performing better, but still making costly "mistakes".
#       We will create a "cost" matrix to assign a penalty to each type of mistake.
#-------------------------------------------------------------------------------------------
# Build matrix..
matrix_dimensions <- list(c("no", "yes"), c("no", "yes"))
names(matrix_dimensions) <- c("predicted", "actual")
matrix_dimensions
# Assign penalties for the various types of errors (4 values), in order:
# predicted "no" -- actual "no"
# predicted "yes" -- actual "no"
# predicted "no" -- actual "yes"
# predicted "yes" -- actual "yes"
# We assume false negatives (predicted no default, loanee defaulted) cost the bank 4 times as much as a missed opportunity.
error_cost <- matrix(c(0, 1, 4, 0), nrow = 2, dimnames = matrix_dimensions)
error_cost
# Apply cost matrix to C5.0 algo using "cost" parameter.
credit_cost <- C5.0(credit_train[-17], credit_train$default, costs = error_cost)
credit_cost_pred <- predict(credit_cost, credit_test)
CrossTable(credit_test$default, credit_cost_pred, prop.chisq = FALSE, prop.r = FALSE, dnn = c("actual default", "predicted default" ))
View(predicted_prob_boost10)
library(tm)
library(NLP)
library(SnowballC)
library(wordcloud)
library(e1071)
library(gmodels)
# import data
sms_raw <- read.csv(url("https://raw.githubusercontent.com/dataspelunking/MLwR/master/Machine%20Learning%20with%20R%20(2nd%20Ed.)/Chapter%2004/sms_spam.csv"), stringsAsFactors = FALSE)
# turn categorical 'type' variable into factor
sms_raw$type <- factor(sms_raw$type)
# get info on imported data
str(sms_raw)
table(sms_raw$type)
# create corpus of SMS messages
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
print(sms_corpus)
# check 1st element of corpus
as.character(sms_corpus[[1]])
# check a list of corpus elements
lapply(sms_corpus[1:3], as.character)
#------------begin cleaning corpus---------------------
# apply lower case transformation to corpus, and check...
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
lapply(sms_corpus_clean[1:3], as.character)
# remove numbers from corpus
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)
# remove stop words from corpus (to, and, but, ...)
sms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords())
# remove punctuation from corpus
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation)
# stem corpus ( learned, learning, learns -> learn)
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
# strip whitespace
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)
#--------------corpus cleaning finalized---------------------
# create a "Document Term Matrix" (DTM). Creates matrix word count per document (SMS).
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
# create training and tests sets
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4170:5559, ]
# create training and tests labels (from raw corpus)
sms_train_labels <- sms_raw[1:4169, ]$type
sms_test_labels <- sms_raw[4170:5559, ]$type
# compare proportion of ham vs spam in both sets
prop.table(table(sms_train_labels))
prop.table(table(sms_test_labels))
#create wordcloud to visulaize frequency of words
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE, random.color = FALSE)
# create and compare wordclouds for ham vs spam (wordcloud applies text prep. processes automatically)
spam <- subset(sms_raw, type == "spam")
ham <- subset(sms_raw, type == "ham")
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 40, scale = c(3, 0.5))
# create a vector with the most frequent words (appearing at least five times)
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
str(sms_freq_words)
# create training and test sets of frequent words
sms_dtm_freq_train <- sms_dtm_train[ , sms_freq_words]
sms_dtm_freq_test <- sms_dtm_test[ , sms_freq_words]
# convert numeric features to categorical (if word appears, "YES", else, "NO")
# This conversion is needed since the Naive Bayes classifier is trained on categorical data
convert_counts <- function(x) {
ifelse(x > 0, "Yes", "No")
}
# create training and test matrices (MARGIN = 2 -- "apply to columns")
# save matrices to CSV file (rows = document #, colums = word instance (yes/no))
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2, convert_counts)
sms_test <- apply(sms_dtm_freq_test, MARGIN = 2, convert_counts)
library(tm)
library(NLP)
library(SnowballC)
library(wordcloud)
library(e1071)
library(gmodels)
# import data
sms_raw <- read.csv(url("https://raw.githubusercontent.com/dataspelunking/MLwR/master/Machine%20Learning%20with%20R%20(2nd%20Ed.)/Chapter%2004/sms_spam.csv"), stringsAsFactors = FALSE)
# turn categorical 'type' variable into factor
sms_raw$type <- factor(sms_raw$type)
# get info on imported data
str(sms_raw)
table(sms_raw$type)
# create corpus of SMS messages
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
print(sms_corpus)
# check 1st element of corpus
as.character(sms_corpus[[1]])
# check a list of corpus elements
lapply(sms_corpus[1:3], as.character)
#------------begin cleaning corpus---------------------
# apply lower case transformation to corpus, and check...
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
lapply(sms_corpus_clean[1:3], as.character)
# remove numbers from corpus
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)
# remove stop words from corpus (to, and, but, ...)
sms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords())
# remove punctuation from corpus
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation)
# stem corpus ( learned, learning, learns -> learn)
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
# strip whitespace
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)
#--------------corpus cleaning finalized---------------------
# create a "Document Term Matrix" (DTM). Creates matrix word count per document (SMS).
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
# create training and tests sets
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4170:5559, ]
# create training and tests labels (from raw corpus)
sms_train_labels <- sms_raw[1:4169, ]$type
sms_test_labels <- sms_raw[4170:5559, ]$type
# compare proportion of ham vs spam in both sets
prop.table(table(sms_train_labels))
prop.table(table(sms_test_labels))
#create wordcloud to visulaize frequency of words
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE, random.color = FALSE)
# create and compare wordclouds for ham vs spam (wordcloud applies text prep. processes automatically)
spam <- subset(sms_raw, type == "spam")
ham <- subset(sms_raw, type == "ham")
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 40, scale = c(3, 0.5))
# create a vector with the most frequent words (appearing at least five times)
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
str(sms_freq_words)
# create training and test sets of frequent words
sms_dtm_freq_train <- sms_dtm_train[ , sms_freq_words]
sms_dtm_freq_test <- sms_dtm_test[ , sms_freq_words]
# convert numeric features to categorical (if word appears, "YES", else, "NO")
# This conversion is needed since the Naive Bayes classifier is trained on categorical data
convert_counts <- function(x) {
ifelse(x > 0, "Yes", "No")
}
# create training and test matrices (MARGIN = 2 -- "apply to columns")
# save matrices to CSV file (rows = document #, colums = word instance (yes/no))
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2, convert_counts)
sms_test <- apply(sms_dtm_freq_test, MARGIN = 2, convert_counts)
write.csv(sms_train, file = "sms_train.csv")
write.csv(sms_test, file = "sms_test.csv")
# Build SMS Classifier Model
sms_classifier <- naiveBayes(sms_train, sms_train_labels)
# Make predicitons using the classifier model on the test data store them in a vector
sms_test_pred <- predict(sms_classifier, sms_test)
# Compare predicitons
CrossTable(sms_test_pred, sms_test_labels, prop.chisq = FALSE, prop.t = FALSE, dnn = c("predicted", "actual"))
# Improve the model by setting a Laplace Estimator of 1
sms_classifier2 <- naiveBayes(sms_train, sms_train_labels, laplace = 1)
sms_test_pred2 <- predict(sms_classifier2, sms_test)
CrossTable(sms_test_pred2, sms_test_labels, prop.chisq = FALSE, prop.t = FALSE, dnn = c("predicted", "actual"))
sms_test_prob <- predict(sms_classifier, sms_test, type = "raw")
View(sms_test)
View(sms_test_prob)
head(sms_test_prob)
sms_test_pred
data.frame(sms_test_labels, sms_test_pred, sms_test_prob)
sms_results <- data.frame(sms_test_labels, sms_test_pred, sms_test_prob)
col_headings <- c('Actual type','Predicted type', 'Prob. Spam','Prob. Ham')
names(sms_results) <- col_headings
View(sms_results)
sms_results <- data.frame(sms_test_labels, sms_test_pred, sms_test_prob)
col_headings <- c('Actual type','Predicted type', 'Prob. Ham','Prob. Spam')
names(sms_results) <- col_headings
View(sms_results)
View(sms_results)
head(sms_test_prob)
# Improve the model by setting a Laplace Estimator of 1
sms_classifier2 <- naiveBayes(sms_train, sms_train_labels, laplace = 1)
sms_test_pred2 <- predict(sms_classifier2, sms_test)
sms_test_prob2 <- predict(sms_classifier2, sms_test, type = "raw")
CrossTable(sms_test_pred2, sms_test_labels, prop.chisq = FALSE, prop.t = FALSE, dnn = c("predicted", "actual"))
#------------ Present findings in a data table. V2 ----------------
sms_results2 <- data.frame(sms_test_labels, sms_test_pred2, sms_test_prob2)
col_headings2 <- c('Actual type','Predicted type', 'Prob. Ham','Prob. Spam')
names(sms_results2) <- col_headings2
View(sms_results2)
install.packages("caret")
library(caret)
confusionMatrix(sms_results2$sms_test_pred2, sms_test_labels, positive = "spam")
confusionMatrix(sms_test_pred2, sms_test_labels, positive = "spam")
install.packages("ROCR")
library(ROCR)
pred <- prediction(predictions = sms_test_prob2$spam, labels = sms_test_labels)
View(sms_test_prob2)
sms_test_prob2$spam
pred <- prediction(predictions = sms_test_prob2[ ,2], labels = sms_test_labels)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf, main = "ROC curve for SMS spam filter", col = "blue", lwd = 3)
pdf("SMS_ROCcurve.pdf")
plot(perf, main = "ROC curve for SMS spam filter", col = "blue", lwd = 3)
dev.off()
setwd("~/Desktop/Machine-Learning/NaiveBayes")
pdf("SMS_ROCcurve.pdf")
plot(perf, main = "ROC curve for SMS spam filter", col = "blue", lwd = 3)
dev.off()
perf.auc <- performance(pred, measure = "auc")
perf.auc <- performance(pred, measure = "auc")
unlist(perf.auc@y.values)
library(C50)
library(gmodels)
# Import data from UCI Machine Learning Repository (http://archive.ics.uci.edu/ml)
# Inspect data
credit <- read.csv(url("https://raw.githubusercontent.com/dataspelunking/MLwR/master/Machine%20Learning%20with%20R%20(2nd%20Ed.)/Chapter%2005/credit.csv"))
str(credit)
# Inspect checking and savings balanace as possible indicators of default
table(credit$checking_balance)
table(credit$savings_balance)
# Obtain summary statistics on loan duration and loan amount
summary(credit$months_loan_duration)
summary(credit$amount)
# Obtain total number of defaults
table(credit$default)
# Obtain random sample to split between training and test data. Set seed.
# Returns a vector with 900 random values
set.seed(123)
train_sample <- sample(1000, 900)
str(train_sample)
# Split data into training and test sets.
# Check to see if both sets have ~ proportion of default loans (~30%, as in original set).
credit_train <- credit[train_sample, ]
credit_test <- credit[-train_sample, ]
prop.table(table(credit_train$default))
prop.table(table(credit_test$default))
# Train model.
# Exclude "default" class from training data (17th column) but supply it as target factor vector for classification (labels).
credit_model <- C5.0(credit_train[-17], credit_train$default)
# credit_model # inspect...
# Inspect Tree's decisions and acquire stats on model performance on the training data.
summary(credit_model)
# Apply model to test data and predict...
# Compare prediction against true results.
# Obtain prediction "confidence" (in terms of probabilities).
credit_pred <- predict(credit_model, credit_test)
predicted_prob <- predict(credit_model, credit_test, type = "prob")
CrossTable(credit_test$default, credit_pred, prop.chisq = FALSE, prop.r = FALSE, dnn = c("actual default", "predicted default" ))
#-------------------------------------------------------------------------------------------
# NOTE: up to this point, the model performed rather poorly at identifying true defaults.
#       The proceeding code is intended to "fine-tune" our model to improve its performance.
#       See "Boosting" for more details.
#-------------------------------------------------------------------------------------------
# We employ the same classifying algorith (C5.0) but we "boost" it by adding "trials".
# The trials parameter indicates the additional number of separate decision trees to use. We set trials = 10.
credit_boost10 <- C5.0(credit_train[-17], credit_train$default, trials = 10)
credit_boost10
# Obtain new model's summary and peformance on the test data. Save to .txt file.
sink("credit_boost10.txt")
summary(credit_boost10)
sink()
# Test new model on test data
# Compare predicition against true results.
credit_boost10_pred <- predict(credit_boost10, credit_test)
predicted_prob_boost10 <- predict(credit_model, credit_test, type = "prob")
CrossTable(credit_test$default, credit_boost10_pred, prop.chisq = FALSE, prop.r = FALSE, dnn = c("actual default", "predicted default" ))
#-------------------------------------------------------------------------------------------
# NOTE: the model is performing better, but still making costly "mistakes".
#       We will create a "cost" matrix to assign a penalty to each type of mistake.
#-------------------------------------------------------------------------------------------
# Build matrix..
matrix_dimensions <- list(c("no", "yes"), c("no", "yes"))
names(matrix_dimensions) <- c("predicted", "actual")
matrix_dimensions
# Assign penalties for the various types of errors (4 values), in order:
# predicted "no" -- actual "no"
# predicted "yes" -- actual "no"
# predicted "no" -- actual "yes"
# predicted "yes" -- actual "yes"
# We assume false negatives (predicted no default, loanee defaulted) cost the bank 4 times as much as a missed opportunity.
error_cost <- matrix(c(0, 1, 4, 0), nrow = 2, dimnames = matrix_dimensions)
error_cost
# Apply cost matrix to C5.0 algo using "cost" parameter.
credit_cost <- C5.0(credit_train[-17], credit_train$default, costs = error_cost)
credit_cost_pred <- predict(credit_cost, credit_test)
CrossTable(credit_test$default, credit_cost_pred, prop.chisq = FALSE, prop.r = FALSE, dnn = c("actual default", "predicted default" ))
#-------------------------------------------------------------------------------------------
# NOTE: compared to the previous models, the version above produces more mistakes (37% vs. 18%).
#       However, the types of mistkes are very different (less false negatives -- 79% vs. 42% & 61%).
#-------------------------------------------------------------------------------------------
library(caret)
folds <- createFolds(credit$default, k = 10)
str(folds)
setwd("~/Desktop/Machine-Learning/DecisionTrees")
library(irr)
install.packages("irr")
library(irr)
library(C50)
library(gmodels)
library(caret)
library(irr)
# Import data from UCI Machine Learning Repository (http://archive.ics.uci.edu/ml)
# Inspect data
credit <- read.csv(url("https://raw.githubusercontent.com/dataspelunking/MLwR/master/Machine%20Learning%20with%20R%20(2nd%20Ed.)/Chapter%2005/credit.csv"))
str(credit)
# Inspect checking and savings balanace as possible indicators of default
table(credit$checking_balance)
table(credit$savings_balance)
# Obtain summary statistics on loan duration and loan amount
summary(credit$months_loan_duration)
summary(credit$amount)
# Obtain total number of defaults
table(credit$default)
# Obtain random sample to split between training and test data. Set seed.
# Returns a vector with 900 random values
set.seed(123)
train_sample <- sample(1000, 900)
str(train_sample)
# Split data into training and test sets.
# Check to see if both sets have ~ proportion of default loans (~30%, as in original set).
credit_train <- credit[train_sample, ]
credit_test <- credit[-train_sample, ]
prop.table(table(credit_train$default))
prop.table(table(credit_test$default))
# Train model.
# Exclude "default" class from training data (17th column) but supply it as target factor vector for classification (labels).
credit_model <- C5.0(credit_train[-17], credit_train$default)
# credit_model # inspect...
# Inspect Tree's decisions and acquire stats on model performance on the training data.
summary(credit_model)
# Apply model to test data and predict...
# Compare prediction against true results.
# Obtain prediction "confidence" (in terms of probabilities).
credit_pred <- predict(credit_model, credit_test)
predicted_prob <- predict(credit_model, credit_test, type = "prob")
CrossTable(credit_test$default, credit_pred, prop.chisq = FALSE, prop.r = FALSE, dnn = c("actual default", "predicted default" ))
#-------------------------------------------------------------------------------------------
# NOTE: up to this point, the model performed rather poorly at identifying true defaults.
#       The proceeding code is intended to "fine-tune" our model to improve its performance.
#       See "Boosting" for more details.
#-------------------------------------------------------------------------------------------
# We employ the same classifying algorith (C5.0) but we "boost" it by adding "trials".
# The trials parameter indicates the additional number of separate decision trees to use. We set trials = 10.
credit_boost10 <- C5.0(credit_train[-17], credit_train$default, trials = 10)
credit_boost10
# Obtain new model's summary and peformance on the test data. Save to .txt file.
sink("credit_boost10.txt")
summary(credit_boost10)
sink()
# Test new model on test data
# Compare predicition against true results.
credit_boost10_pred <- predict(credit_boost10, credit_test)
predicted_prob_boost10 <- predict(credit_model, credit_test, type = "prob")
CrossTable(credit_test$default, credit_boost10_pred, prop.chisq = FALSE, prop.r = FALSE, dnn = c("actual default", "predicted default" ))
#-------------------------------------------------------------------------------------------
# NOTE: the model is performing better, but still making costly "mistakes".
#       We will create a "cost" matrix to assign a penalty to each type of mistake.
#-------------------------------------------------------------------------------------------
# Build matrix..
matrix_dimensions <- list(c("no", "yes"), c("no", "yes"))
names(matrix_dimensions) <- c("predicted", "actual")
matrix_dimensions
# Assign penalties for the various types of errors (4 values), in order:
# predicted "no" -- actual "no"
# predicted "yes" -- actual "no"
# predicted "no" -- actual "yes"
# predicted "yes" -- actual "yes"
# We assume false negatives (predicted no default, loanee defaulted) cost the bank 4 times as much as a missed opportunity.
error_cost <- matrix(c(0, 1, 4, 0), nrow = 2, dimnames = matrix_dimensions)
error_cost
# Apply cost matrix to C5.0 algo using "cost" parameter.
credit_cost <- C5.0(credit_train[-17], credit_train$default, costs = error_cost)
credit_cost_pred <- predict(credit_cost, credit_test)
CrossTable(credit_test$default, credit_cost_pred, prop.chisq = FALSE, prop.r = FALSE, dnn = c("actual default", "predicted default" ))
#-------------------------------------------------------------------------------------------
# NOTE: compared to the previous models, the version above produces more mistakes (37% vs. 18%).
#       However, the types of mistkes are very different (less false negatives -- 79% vs. 42% & 61%).
#-------------------------------------------------------------------------------------------
#------------ K-Folds Cross Validation ------------
set.seed(123)
folds <- createFolds(credit$default, k = 10)
str(folds)
cv_results <- lapply(folds, function(x) {
credit_train <- credit[-x, ]
credit_test <- credit[x, ]
credit_model <- C5.0(default ~ ., data = credit_train)
credit_pred <- predict(credit_model, credit_test)
credit_actual <- credit_test$default
kappa <- kappa2(data.frame(credit_actual, credit_pred))$value
return(kappa)
})
str(cv_results)
str(cv_results)
mean(unlist(cv_results))
install.packages("XGBoost")
install.packages("xgboost")
